%%cu

#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include "/content/drive/MyDrive/Colab Notebooks/hs/RTree.h"

struct Punto {
    int id;
    int x;
    int y;
    int hilbert;
};

struct Rect {
  __host__ __device__ Rect() {}

  int min[2];
  int max[2];
};

typedef RTree<int, int, 2, float> OneTree;

void mergesort(Punto*, long, dim3, dim3);
__global__ void gpu_mergesort(Punto*, Punto*, long, long, long, dim3*, dim3*);
__device__ void gpu_bottomUpMerge(Punto*, Punto*, long, long, long);

/*** 
    1- Leer archivo con datos - datos que se usan en índices (On the Generation of Spatiotemporal Datasets - GSTD)
    2- Repartir entre los cores para calcular valor hilbert
    3- Ordenar según ese valor
    4- Repartir entre los cores para armar árbol r
    5- Generar árbol r en cada core y devolver la raiz
    6- Generar un árbol principal con todas las raíces obtenidas
***/
#define LIMITE 100
#define TAPP 25 //Tamaño de árbol por procesador
#define CANTPROC 4
#define TMAXNODES 8

__device__ OneTree t[CANTPROC];//*trees[CANTPROC];
__device__ Rect rects[CANTPROC];

__device__ void rot(int n, int *x, int *y, int rx, int ry) {
    if (ry == 0) {
        if (rx == 1) {
            *x = n-1 - *x;
            *y = n-1 - *y;
        }

        //Intercambiar x e y
        int t  = *x;
        *x = *y;
        *y = t;
    }
}

__global__ void calc_hilbert(Punto *p){
    int tid = threadIdx.x;
    int rx, ry, s, d=0;
    int n = 128, x = p[tid].x, y = p[tid].y; //n -> corresponde al valor máximo que pueden tomar x-1 o y-1. Además, debe ser múltiplo de 2
    for (s=n/2; s>0; s/=2) {
        rx = (x & s) > 0;
        ry = (y & s) > 0;
        d += s * s * ((3 * rx) ^ ry);
        rot(n, &x, &y, rx, ry);
    }
    //printf ("%d", d);
    /*if (tid == 0){
        printf ("***Valores para thread 0: x = %d, y = %d, h = %d\n", p[tid].x, p[tid].y, d);
    }*/
    p[tid].hilbert = d;
}

void mergesort(Punto* data, long size, dim3 threadsPerBlock, dim3 blocksPerGrid) {

    //
    // Allocate two arrays on the GPU
    // we switch back and forth between them during the sort
    //
    Punto* D_data;
    Punto* D_swp;
    dim3* D_threads;
    dim3* D_blocks;
    
    // Actually allocate the two arrays
    cudaMalloc((void**) &D_data, size * sizeof(Punto));
    cudaMalloc((void**) &D_swp, size * sizeof(Punto));

    // Copy from our input list into the first array
    cudaMemcpy(D_data, data, size * sizeof(Punto), cudaMemcpyHostToDevice);
 
    //
    // Copy the thread / block info to the GPU as well
    //
    cudaMalloc((void**) &D_threads, sizeof(dim3));
    cudaMalloc((void**) &D_blocks, sizeof(dim3));

    cudaMemcpy(D_threads, &threadsPerBlock, sizeof(dim3), cudaMemcpyHostToDevice);
    cudaMemcpy(D_blocks, &blocksPerGrid, sizeof(dim3), cudaMemcpyHostToDevice);

    Punto* A = D_data;
    Punto* B = D_swp;

    long nThreads = threadsPerBlock.x * threadsPerBlock.y * threadsPerBlock.z *
                    blocksPerGrid.x * blocksPerGrid.y * blocksPerGrid.z;

    //
    // Slice up the list and give pieces of it to each thread, letting the pieces grow
    // bigger and bigger until the whole list is sorted
    //
    for (int width = 2; width < (size << 1); width <<= 1) {
        long slices = size / ((nThreads) * width) + 1;

        // Actually call the kernel
        gpu_mergesort<<<blocksPerGrid, threadsPerBlock>>>(A, B, size, width, slices, D_threads, D_blocks);

        // Switch the input / output arrays instead of copying them around
        A = A == D_data ? D_swp : D_data;
        B = B == D_data ? D_swp : D_data;
    }

    //
    // Get the list back from the GPU
    //
    cudaMemcpy(data, A, size * sizeof(Punto), cudaMemcpyDeviceToHost);
    
    // Free the GPU memory
    cudaFree(A);
    cudaFree(B);
}

// GPU helper function
// calculate the id of the current thread
__device__ unsigned int getIdx(dim3* threads, dim3* blocks) {
    int x;
    return threadIdx.x +
           threadIdx.y * (x  = threads->x) +
           threadIdx.z * (x *= threads->y) +
           blockIdx.x  * (x *= threads->z) +
           blockIdx.y  * (x *= blocks->z) +
           blockIdx.z  * (x *= blocks->y);
}

//
// Perform a full mergesort on our section of the data.
//
__global__ void gpu_mergesort(Punto* source, Punto* dest, long size, long width, long slices, dim3* threads, dim3* blocks) {
    unsigned int idx = getIdx(threads, blocks);
    long start = width*idx*slices, 
         middle, 
         end;

    for (long slice = 0; slice < slices; slice++) {
        if (start >= size)
            break;

        middle = min(start + (width >> 1), size);
        end = min(start + width, size);
        gpu_bottomUpMerge(source, dest, start, middle, end);
        start += width;
    }
}

//
// Finally, sort something
// gets called by gpu_mergesort() for each slice
//
__device__ void gpu_bottomUpMerge(Punto* source, Punto* dest, long start, long middle, long end) {
    long i = start;
    long j = middle;
    for (long k = start; k < end; k++) {
        if (i < middle && (j >= end || source[i].hilbert < source[j].hilbert)) {
            dest[k] = source[i];
            i++;
        } else {
            dest[k] = source[j];
            j++;
        }
    }
}

__host__ __device__ bool MySearchCallback1(int id){
  printf("Hit data rect %d\n", id);
  return true; // keep going
}

__global__ void gen_arbol(Punto* source){
    #include <stdio.h>
    printf(" exit proc ");
    int tid = threadIdx.x;
    int first = 1;
    t[tid].Inicializar();
    for(int i = tid * (LIMITE / CANTPROC); i < tid * (LIMITE / CANTPROC) + TAPP; i++){
        Rect r;// = Rect (source[i].x,source[i].y,source[i].x,source[i].y);
        r.min[0] = r.max[0] = source[i].x;
        r.min[1] = r.max[1] = source[i].y;
        t[tid].Insert(r.min, r.max, source[i].id, first); // Note, all values including zero are fine in this version
        first = 0;
    }
    __syncthreads();

    t[tid].GetRoot(&(rects[tid].min[0]), &(rects[tid].min[1]), &(rects[tid].max[0]), &(rects[tid].max[1]));
}

int main(){
    Punto puntos[LIMITE];
    int nhits;

    Punto* dev_puntos;
    Punto* dev_arbol;

// 1- Leer archivo con datos
    int i = 0;
    char cadena1[LIMITE];
    FILE* fichero;
    fichero = fopen("/content/drive/MyDrive/Colab Notebooks/datos/datosentrada.csv", "rt");
    fgets (cadena1, LIMITE, fichero); //encabezados
    while (fgets(cadena1, LIMITE, fichero)) {
        char* token = strtok(cadena1, ";");
        puntos[i].id = atoi(token);
        token = strtok(NULL, ";");
        puntos[i].x = (int)trunc(atof(token) * 1000) % 100;
        token = strtok(NULL, ";");
        puntos[i].y = (int)trunc(atof(token) * 1000) % 100;
        //printf ("%d - %d \n", puntos[i].x, puntos[i].y);
        i++;
    }
    fclose(fichero);
// FIN 1

// 2- Repartir entre los cores para calcular valor hilbert
    cudaMalloc((void**)&dev_puntos , LIMITE * sizeof(Punto));
    cudaMemcpy(dev_puntos, puntos, LIMITE * sizeof(Punto), cudaMemcpyHostToDevice);
    calc_hilbert<<<1,LIMITE>>>(dev_puntos);
    cudaMemcpy(puntos, dev_puntos, LIMITE * sizeof(Punto), cudaMemcpyDeviceToHost);

    cudaFree(dev_puntos);
// FIN 2

// 3- Ordenar según ese valor
    dim3 threadsPerBlock;
    dim3 blocksPerGrid;

    threadsPerBlock.x = LIMITE;
    threadsPerBlock.y = 1;
    threadsPerBlock.z = 1;

    blocksPerGrid.x = 1;
    blocksPerGrid.y = 1;
    blocksPerGrid.z = 1;

    // merge-sort the data
    mergesort(puntos, LIMITE, threadsPerBlock, blocksPerGrid);
// FIN 3

// 4- Repartir entre los cores para armar árbol r

    OneTree treesCPU[CANTPROC];

cudaError_t err;

    cudaMalloc((void**)&dev_arbol, LIMITE * sizeof(Punto));

    cudaMemcpy(dev_arbol, puntos, LIMITE * sizeof(Punto), cudaMemcpyHostToDevice);

    gen_arbol<<<1, CANTPROC>>>(dev_arbol); 
    cudaFree(dev_arbol);
     err = cudaGetLastError();

     if ( err != cudaSuccess ){
        printf("CUDA Error1: %s\n", cudaGetErrorString(err));       

        // Possibly: exit(-1) if program cannot continue....
     }
   
    return 0;
}
